{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrXS9kuaZlkb7/18TT4OWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhtarali34/AI-Resume-Screening-/blob/main/NakbaTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv5ODotZ7gIJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score,\n",
        "    precision_score, recall_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wCi3f1yRLKM",
        "outputId": "61ef93c2-4a3c-4ae6-b870-d6e8262752bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7U_U4t4ROxh",
        "outputId": "91b750f0-cc5e-4bf3-90a6-3a12d0480336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9de0ee2",
        "outputId": "ff9c86ab-7056-4caa-efcd-2a42c7780107"
      },
      "source": [
        "# =======================\n",
        "# Mount Google Drive\n",
        "# =======================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# =======================\n",
        "# Imports\n",
        "# =======================\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# =======================\n",
        "# Paths\n",
        "# =======================\n",
        "ZIP_PATH = \"/content/drive/MyDrive/NakbaData/ImgClassDevPhasePublic.zip\"\n",
        "EXTRACT_ROOT = \"/content/NakbaData\"\n",
        "\n",
        "# =======================\n",
        "# Extract dataset\n",
        "# =======================\n",
        "os.makedirs(EXTRACT_ROOT, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(EXTRACT_ROOT)\n",
        "\n",
        "# =======================\n",
        "# Locate dataset root\n",
        "# =======================\n",
        "DATA_ROOT = None\n",
        "for root, _, files in os.walk(EXTRACT_ROOT):\n",
        "    if {\"Train.xlsx\", \"Val.xlsx\"}.issubset(files):\n",
        "        DATA_ROOT = root\n",
        "        break\n",
        "\n",
        "if DATA_ROOT is None:\n",
        "    raise FileNotFoundError(\"Dataset root not found\")\n",
        "\n",
        "# =======================\n",
        "# Directories\n",
        "# =======================\n",
        "TRAIN_IMG_DIR = os.path.join(DATA_ROOT, \"Train\")\n",
        "VAL_IMG_DIR   = os.path.join(DATA_ROOT, \"Val\")\n",
        "\n",
        "# =======================\n",
        "# Load labels\n",
        "# =======================\n",
        "train_df = pd.read_excel(os.path.join(DATA_ROOT, \"Train.xlsx\"))\n",
        "val_df   = pd.read_excel(os.path.join(DATA_ROOT, \"Val.xlsx\"))\n",
        "\n",
        "# =======================\n",
        "# Label encoding\n",
        "# =======================\n",
        "LABEL_MAP = {\n",
        "    \"not_destruction\": 0,\n",
        "    \"destruction\": 1\n",
        "}\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].map(LABEL_MAP)\n",
        "val_df[\"label\"]   = val_df[\"label\"].map(LABEL_MAP)\n",
        "\n",
        "print(\"Train samples:\", len(train_df))\n",
        "print(\"Val samples:\", len(val_df))\n",
        "print(train_df[\"label\"].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train samples: 1400\n",
            "Val samples: 199\n",
            "label\n",
            "0    906\n",
            "1    494\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90629032"
      },
      "source": [
        "class NakbaDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.df.loc[idx, \"name\"]   # ✅ FIXED\n",
        "        label = self.df.loc[idx, \"label\"]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.2, 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "WTtPnPFLRZ6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NakbaDataset(train_df, TRAIN_IMG_DIR, train_transform)\n",
        "val_dataset   = NakbaDataset(val_df, VAL_IMG_DIR, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "VgdXvV13RZ_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(name):\n",
        "    if name == \"resnet50\":\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "\n",
        "    elif name == \"mobilenet\":\n",
        "        model = models.mobilenet_v2(pretrained=True)\n",
        "        model.classifier[1] = nn.Linear(model.last_channel, 1)\n",
        "\n",
        "    elif name == \"efficientnet\":\n",
        "        model = models.efficientnet_b0(pretrained=True)\n",
        "        model.classifier[1] = nn.Linear(\n",
        "            model.classifier[1].in_features, 1\n",
        "        )\n",
        "\n",
        "    return model.to(device)\n"
      ],
      "metadata": {
        "id": "bl8d0zGlRaC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce_loss = self.bce(logits, targets)\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "        return focal_loss.mean()\n"
      ],
      "metadata": {
        "id": "JdLSuzD6RaGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, threshold=0.5):\n",
        "    model.eval()\n",
        "    probs, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(images).squeeze()\n",
        "            p = torch.sigmoid(outputs)\n",
        "\n",
        "            probs.extend(p.cpu().numpy())\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    probs = np.array(probs)\n",
        "    labels = np.array(labels)\n",
        "    preds = (probs > threshold).astype(int)\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds),\n",
        "        \"cm\": confusion_matrix(labels, preds)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "x_swuBNpRaJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_threshold(model, loader):\n",
        "    best_f1, best_t = 0, 0.5\n",
        "    for t in np.arange(0.3, 0.71, 0.05):\n",
        "        metrics = evaluate(model, loader, t)\n",
        "        if metrics[\"f1\"] > best_f1:\n",
        "            best_f1, best_t = metrics[\"f1\"], t\n",
        "    return best_t, best_f1\n"
      ],
      "metadata": {
        "id": "f_7F8cDERaMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njBhQGBuaoUX",
        "outputId": "5c44c068-fe96-49a6-da47-bff1563bb56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['name', 'label'], dtype='object')\n",
            "                                                name  label\n",
            "0  3207587610565662721_1379830319_320758761056566...      0\n",
            "1  3208072596685860139_6036075308_320807259668586...      0\n",
            "2  3208234742834392153_53084429579_32082347428343...      0\n",
            "3  3208236458824789277_37252106111_32082364588247...      1\n",
            "4  3208376724789487084_1379830319_320837672478948...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, loss_fn, epochs=20):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", patience=3\n",
        "    )\n",
        "\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "\n",
        "        for images, targets in tqdm(train_loader):\n",
        "            images = images.to(device)\n",
        "            targets = targets.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images).squeeze()\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        metrics = evaluate(model, val_loader)\n",
        "        scheduler.step(metrics[\"f1\"])\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        print(\"Loss:\", running_loss / len(train_loader))\n",
        "        print(metrics)\n",
        "\n",
        "        if metrics[\"f1\"] > best_f1:\n",
        "            best_f1 = metrics[\"f1\"]\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(\"✅ Best model saved\")\n"
      ],
      "metadata": {
        "id": "TRxlot9mRaPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(\"resnet50\")\n",
        "loss_fn = FocalLoss()\n",
        "\n",
        "train_model(model, loss_fn, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pgbY4w6RaSW",
        "outputId": "01ca592b-83b3-4fcc-df3c-43372553a778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 44/44 [00:32<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Loss: 0.13698747982694345\n",
            "{'f1': 0.7910909348843808, 'acc': 0.8140703517587939, 'precision': 0.7619047619047619, 'recall': 0.6857142857142857, 'cm': array([[114,  15],\n",
            "       [ 22,  48]])}\n",
            "✅ Best model saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:33<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "Loss: 0.09929138574410569\n",
            "{'f1': 0.6378779395296752, 'acc': 0.7386934673366834, 'precision': 0.875, 'recall': 0.3, 'cm': array([[126,   3],\n",
            "       [ 49,  21]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:32<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "Loss: 0.10042552887038751\n",
            "{'f1': 0.7846320346320346, 'acc': 0.8090452261306532, 'precision': 0.7580645161290323, 'recall': 0.6714285714285714, 'cm': array([[114,  15],\n",
            "       [ 23,  47]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:32<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "Loss: 0.08847512745044449\n",
            "{'f1': 0.7709771918811468, 'acc': 0.7788944723618091, 'precision': 0.6413043478260869, 'recall': 0.8428571428571429, 'cm': array([[96, 33],\n",
            "       [11, 59]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:35<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "Loss: 0.08134110094132749\n",
            "{'f1': 0.7760440429479576, 'acc': 0.8140703517587939, 'precision': 0.851063829787234, 'recall': 0.5714285714285714, 'cm': array([[122,   7],\n",
            "       [ 30,  40]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:32<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6\n",
            "Loss: 0.06956643526527015\n",
            "{'f1': 0.7615301889691197, 'acc': 0.8040201005025126, 'precision': 0.8444444444444444, 'recall': 0.5428571428571428, 'cm': array([[122,   7],\n",
            "       [ 32,  38]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7\n",
            "Loss: 0.06511189712380822\n",
            "{'f1': 0.7831719128329298, 'acc': 0.8190954773869347, 'precision': 0.8541666666666666, 'recall': 0.5857142857142857, 'cm': array([[122,   7],\n",
            "       [ 29,  41]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8\n",
            "Loss: 0.0607830969278108\n",
            "{'f1': 0.8040695766327535, 'acc': 0.8341708542713567, 'precision': 0.8627450980392157, 'recall': 0.6285714285714286, 'cm': array([[122,   7],\n",
            "       [ 26,  44]])}\n",
            "✅ Best model saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9\n",
            "Loss: 0.06206415627490391\n",
            "{'f1': 0.785988631126133, 'acc': 0.8241206030150754, 'precision': 0.8888888888888888, 'recall': 0.5714285714285714, 'cm': array([[124,   5],\n",
            "       [ 30,  40]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10\n",
            "Loss: 0.05681423080915755\n",
            "{'f1': 0.7760440429479576, 'acc': 0.8140703517587939, 'precision': 0.851063829787234, 'recall': 0.5714285714285714, 'cm': array([[122,   7],\n",
            "       [ 30,  40]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11\n",
            "Loss: 0.05548336699774319\n",
            "{'f1': 0.7787249814677539, 'acc': 0.8190954773869347, 'precision': 0.8863636363636364, 'recall': 0.5571428571428572, 'cm': array([[124,   5],\n",
            "       [ 31,  39]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12\n",
            "Loss: 0.05190151840956374\n",
            "{'f1': 0.8040695766327535, 'acc': 0.8341708542713567, 'precision': 0.8627450980392157, 'recall': 0.6285714285714286, 'cm': array([[122,   7],\n",
            "       [ 26,  44]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13\n",
            "Loss: 0.05166570990431038\n",
            "{'f1': 0.8040695766327535, 'acc': 0.8341708542713567, 'precision': 0.8627450980392157, 'recall': 0.6285714285714286, 'cm': array([[122,   7],\n",
            "       [ 26,  44]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14\n",
            "Loss: 0.049945054279470984\n",
            "{'f1': 0.830886446886447, 'acc': 0.8542713567839196, 'precision': 0.8727272727272727, 'recall': 0.6857142857142857, 'cm': array([[122,   7],\n",
            "       [ 22,  48]])}\n",
            "✅ Best model saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15\n",
            "Loss: 0.05380384141409939\n",
            "{'f1': 0.817620103473762, 'acc': 0.8442211055276382, 'precision': 0.8679245283018868, 'recall': 0.6571428571428571, 'cm': array([[122,   7],\n",
            "       [ 24,  46]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16\n",
            "Loss: 0.053610369182107126\n",
            "{'f1': 0.817620103473762, 'acc': 0.8442211055276382, 'precision': 0.8679245283018868, 'recall': 0.6571428571428571, 'cm': array([[122,   7],\n",
            "       [ 24,  46]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17\n",
            "Loss: 0.05411439782685854\n",
            "{'f1': 0.8242877325170709, 'acc': 0.8492462311557789, 'precision': 0.8703703703703703, 'recall': 0.6714285714285714, 'cm': array([[122,   7],\n",
            "       [ 23,  47]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18\n",
            "Loss: 0.05087910393591632\n",
            "{'f1': 0.817620103473762, 'acc': 0.8442211055276382, 'precision': 0.8679245283018868, 'recall': 0.6571428571428571, 'cm': array([[122,   7],\n",
            "       [ 24,  46]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:30<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19\n",
            "Loss: 0.048673064574937926\n",
            "{'f1': 0.830886446886447, 'acc': 0.8542713567839196, 'precision': 0.8727272727272727, 'recall': 0.6857142857142857, 'cm': array([[122,   7],\n",
            "       [ 22,  48]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:29<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20\n",
            "Loss: 0.0515242515724491\n",
            "{'f1': 0.8374183006535948, 'acc': 0.8592964824120602, 'precision': 0.875, 'recall': 0.7, 'cm': array([[122,   7],\n",
            "       [ 21,  49]])}\n",
            "✅ Best model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "robust_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.GaussianBlur(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],\n",
        "                         [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "robust_dataset = NakbaDataset(val_df, VAL_IMG_DIR, robust_transform)\n",
        "robust_loader = DataLoader(robust_dataset, batch_size=32)\n",
        "\n",
        "robust_metrics = evaluate(model, robust_loader)\n",
        "print(\"Robustness Macro F1:\", robust_metrics[\"f1\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNxyoOg5RaVb",
        "outputId": "2b386a4d-1c85-4b47-f098-301a4deeac1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robustness Macro F1: 0.8242877325170709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best trained model\n",
        "model = get_model(\"resnet50\")\n",
        "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oOOUJrgRaYs",
        "outputId": "d798048c-2350-4936-c0e7-0b67b1c061f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on VALIDATION SET (no shuffle)\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _ in tqdm(val_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        outputs = model(images).squeeze()\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = (probs > 0.5).long().cpu().numpy()\n",
        "\n",
        "        predictions.extend(preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM6gC0p3Rab8",
        "outputId": "1f6542ab-d428-41c4-c42a-13a7c1583ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:04<00:00,  1.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to TEXT labels\n",
        "INV_LABEL_MAP = {\n",
        "    0: \"not_destruction\",\n",
        "    1: \"destruction\"\n",
        "}\n",
        "\n",
        "predicted_labels = [INV_LABEL_MAP[p] for p in predictions]\n"
      ],
      "metadata": {
        "id": "6_WWuRpjRafd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create predictions.csv (MATCH XLSX ORDER)\n",
        "submission_df = pd.DataFrame({\n",
        "    \"name\": val_df[\"name\"].values,\n",
        "    \"label\": predicted_labels\n",
        "})\n"
      ],
      "metadata": {
        "id": "41wnhYznRai8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2Xusa9i3RanO",
        "outputId": "12aded55-539e-4bbf-c77f-27b308032e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                name            label\n",
              "0  3208719757836632197_6193457977_320871975783663...  not_destruction\n",
              "1  3208891248927756376_47097946384_32088912489277...  not_destruction\n",
              "2  3209223995252929700_1476949184_386898870_33482...      destruction\n",
              "3  3210450343581475802_51621841193_32104503435814...      destruction\n",
              "4  3212537336353015700_44724541933_32125373363530...      destruction"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3278a671-2532-4d9d-877e-c348a84a32a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3208719757836632197_6193457977_320871975783663...</td>\n",
              "      <td>not_destruction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3208891248927756376_47097946384_32088912489277...</td>\n",
              "      <td>not_destruction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3209223995252929700_1476949184_386898870_33482...</td>\n",
              "      <td>destruction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3210450343581475802_51621841193_32104503435814...</td>\n",
              "      <td>destruction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3212537336353015700_44724541933_32125373363530...</td>\n",
              "      <td>destruction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3278a671-2532-4d9d-877e-c348a84a32a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3278a671-2532-4d9d-877e-c348a84a32a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3278a671-2532-4d9d-877e-c348a84a32a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "submission_df",
              "summary": "{\n  \"name\": \"submission_df\",\n  \"rows\": 199,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 199,\n        \"samples\": [\n          \"3444850773147915473_63510247872_457331725_361148007048962_7633754435542346437_n.png\",\n          \"3249477858680348335_1447521769_3249477858680348335_1447521769_405492878_1102244877609690_7274686915403650846_n.png\",\n          \"3554918246735337970_53381015894_3554918246735337970_53381015894_475278926_497797560015475_5362316082518030743_n.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"destruction\",\n          \"not_destruction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drive directory (must exist)\n",
        "SUBMISSION_DIR = \"/content/drive/MyDrive/NakbaTask/submission\"\n",
        "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
        "\n",
        "CSV_PATH = os.path.join(SUBMISSION_DIR, \"predictions.csv\")\n",
        "\n",
        "# Create predictions.csv (MATCH XLSX ORDER)\n",
        "submission_df = pd.DataFrame({\n",
        "    \"name\": val_df[\"name\"].values,\n",
        "    \"label\": predicted_labels\n",
        "})\n",
        "\n",
        "# Save CSV (UTF-8)\n",
        "submission_df.to_csv(CSV_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ predictions.csv saved to Drive:\", CSV_PATH)"
      ],
      "metadata": {
        "id": "q8xpIqTPRarR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993a6812-00fa-4ce5-80e5-b772f4e96d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ predictions.csv saved to Drive: /content/drive/MyDrive/NakbaTask/submission/predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "ZIP_PATH = os.path.join(SUBMISSION_DIR, \"predictions.zip\")\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipf.write(CSV_PATH, arcname=\"predictions.csv\")\n",
        "\n",
        "print(\"✅ predictions.zip saved to Drive:\", ZIP_PATH)"
      ],
      "metadata": {
        "id": "35tmW2QGRauW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474c708e-78c9-45fe-fae4-ef504b2b6e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ predictions.zip saved to Drive: /content/drive/MyDrive/NakbaTask/submission/predictions.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEwr3XD9s-Ik",
        "outputId": "a88c03b2-d818-4f5b-fd3a-3baae3243a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A0OCInitWKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}